---
title: "ITEC621 Project"
format: docx
editor: visual
echo: true
---

## Quarto

Quarto enables you to weave together content and executable code into a finished document. To learn more about Quarto see <https://quarto.org>.

## Running Code

load dataset

```{r}
Shopping <- read.table("online_shoppers_intention.csv", header = TRUE, sep = ",")
```

4.1 Descriptive statistics of key variables (i.e., means and standard deviations or variances)

```{r}

library(dplyr)

# Convert categorical variables to factors
Shopping$Month <- as.factor(Shopping$Month)
Shopping$VisitorType <- as.factor(Shopping$VisitorType)
Shopping$OperatingSystems <- as.factor(Shopping$OperatingSystems)
Shopping$Browser <- as.factor(Shopping$Browser)
Shopping$Region <- as.factor(Shopping$Region)
Shopping$TrafficType <- as.factor(Shopping$TrafficType)

head(Shopping)
summary(Shopping)

```

mean and standard dev. for numeric varaibles

```{r}
library(psych)
describe(Shopping)

# Mean Product Duration by Revenue 
tapply(Shopping$ProductRelated_Duration, Shopping$Revenue, mean, na.rm = TRUE)
tapply(Shopping$ProductRelated_Duration, Shopping$Revenue, sd, na.rm = TRUE)

# Bounce Rates by Revenue 
tapply(Shopping$BounceRates, Shopping$Revenue, mean, na.rm = TRUE)
tapply(Shopping$BounceRates, Shopping$Revenue, sd, na.rm = TRUE)

# Exit Rates by Revenue 
tapply(Shopping$ExitRates, Shopping$Revenue, mean, na.rm = TRUE)

# Page Values by Revenue 
tapply(Shopping$PageValues, Shopping$Revenue, mean, na.rm = TRUE)

# Special Day Score by Revenue 
tapply(Shopping$SpecialDay, Shopping$Revenue, mean, na.rm = TRUE)
```

categorical variables frequency

```{r}
 #Visitor Type
table(Shopping$VisitorType)
table(Shopping$VisitorType, Shopping$Revenue)

# Weekend visits
table(Shopping$Weekend)
table(Shopping$Weekend, Shopping$Revenue)

# Month of visit
table(Shopping$Month)
table(Shopping$Month, Shopping$Revenue)
```

4.2 Distribution of the outcome variable (e.g., normal, skewed, Poisson, binary, etc.-BINARY

```{r}
barplot(table(Shopping$Revenue),
        main = "Distribution of Purchase Outcome",
        ylab = "Number of Sessions",
        names.arg = c("No Purchase", "Purchase"),
col = c("darkred", "darkgreen"))

```

4.2 Distribution of the outcome variable (e.g., normal, skewed, Poisson, binary, etc.-BINARY

```{r}

#Previous Code for simple heatmap (Victoria)

numeric_vars <- Shopping[, c("ProductRelated_Duration", "BounceRates", "ExitRates", "PageValues", "SpecialDay")]

# Correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")
print(cor_matrix)

# Simple heatmap
heatmap(cor_matrix, main = "Correlation Heatmap of Numeric Variables")

# ______Select relevant numeric variables______
numeric_vars <- Shopping[, c("ProductRelated_Duration", "BounceRates", "ExitRates", "PageValues", "SpecialDay")]

# Correlation matrix
cor_matrix <- cor(numeric_vars, use = "complete.obs")
print(cor_matrix)

library(pheatmap)

# Simple heatmap
pheatmap(cor_matrix,
         main = "Correlation Heatmap of Numeric Variables",
         fontsize_row = 12,      # Increase row label size
         fontsize_col = 12,      # Increase column label size
         display_numbers = TRUE, # Optional: show correlation values
         clustering_method = "complete",
         angle_col = 45)


```

4.2 Distribution of the outcome variable (e.g., normal, skewed, Poisson, binary, etc.-BINARY

```{r}
barplot(table(Shopping$Revenue),
        main = "Distribution of Purchase Outcome",
        ylab = "Number of Sessions",
        names.arg = c("No Purchase", "Purchase"),
col = c("darkred", "darkgreen"))

```

4.2 Distribution of the outcome variable (e.g., normal, skewed, Poisson, binary, etc.-BINARY

```{r}
barplot(table(Shopping$Revenue),
        main = "Distribution of Purchase Outcome",
        ylab = "Number of Sessions",
        names.arg = c("No Purchase", "Purchase"),
col = c("darkred", "darkgreen"))

```

4.3 Correlation and co-variation analysis (e.g., correlation analysis, ANOVA, chi-square tests of

independence).

```{r}
# numeric variables
Shopping.num <- Shopping[, c("ProductRelated_Duration", "BounceRates", "ExitRates", "PageValues", "SpecialDay")]

# Correlation plots
Shopping.cor <- cor(Shopping.num)
library(corrplot)
corrplot(Shopping.cor,
order= "hclust",
method= "number")

corrplot(Shopping.cor,
order= "hclust",
method= "ellipse")

```

Chi-square Test (cat v. cat)

```{r}
# Visitor Type vs Revenue
table_vt <- table(Shopping$VisitorType, Shopping$Revenue)
chisq.test(table_vt)

# Weekend vs Revenue
table_weekend <- table(Shopping$Weekend, Shopping$Revenue)
chisq.test(table_weekend)

# Month vs Revenue
table_month <- table(Shopping$Month, Shopping$Revenue)
chisq.test(table_month)

```

ANOVA Test (num vs. cat)-product duration differ by Revenue?

```{r}

anova_result <- aov(ProductRelated_Duration ~ Revenue, data = Shopping)
summary(anova_result)

```

5.2 Initial Logistic Regression Model

```{r}

# Fit the model

logit_model <- glm(Revenue ~ ProductRelated_Duration + BounceRates + ExitRates + VisitorType + Weekend + Month, 
                   family = binomial, data = Shopping)

# Summary of results
summary(logit_model)

# Multicollinearity Test (VIF)

library(car)
vif(logit_model)

```

Balance the data
```{r}
set.seed(1) # Random seed

# Separate the positive and negative classes
Shopping.1 <- subset(Shopping, Revenue == TRUE)  # Purchases
Shopping.0 <- subset(Shopping, Revenue == FALSE) # Non-purchases

cat("Positive observations = ", nrow(Shopping.1), "\n")
cat("Negative observations = ", nrow(Shopping.0), "\n")

# Oversample positives to match the number of negatives
Shopping.1.oversampled <- Shopping.1[sample(nrow(Shopping.1), size = nrow(Shopping.0), replace = TRUE), ]
cat("Oversampled positive observations = ", nrow(Shopping.1.oversampled), "\n")

# Combine oversampled positives with original negatives
Shopping.bal <- rbind(Shopping.0, Shopping.1.oversampled)
cat("Total balanced observations = ", nrow(Shopping.bal), "\n")

# Check new class balance
table(Shopping.bal$Revenue)

```

Logit model with balanced data

```{r}
logit_model_bal <- glm(Revenue ~ ProductRelated_Duration + BounceRates + ExitRates + VisitorType + Weekend + Month, 
                   family = binomial, data = Shopping.bal)

# Summary of results
summary(logit_model_bal)

# Multicollinearity Check (again) for balanced data
vif(logit_model_bal)

```

LASSO Model Unbalanced Specification, Cross Validation, and Optimal Lambda/Deviance Extraction

```{r}

library(glmnet)

x1 <- model.matrix(Revenue ~ ProductRelated_Duration + BounceRates + ExitRates + 
                   VisitorType + Weekend + Month, data = Shopping)[, -1]
y1 <- Shopping$Revenue

# Fit LASSO model
lasso_model1 <- glmnet(x1, y1, alpha = 1, family = "binomial")
plot(lasso_model1, main = "LASSO Coefficient Path - Spec 1")

# Cross-validation
set.seed(1)
lasso_cv1 <- cv.glmnet(x1, y1, alpha = 1, family = "binomial")
plot(lasso_cv1)

# Extract optimal lambda and deviance
lasso_lambda1 <- lasso_cv1$lambda.min
lasso_coef1 <- coef(lasso_cv1, s = lasso_lambda1)

round(cbind("Best Lambda" = lasso_lambda1,
            "Best Log Lambda" = log(lasso_lambda1),
            "Best 10FCV" = min(lasso_cv1$cvm)), 5)
```

LASSO Model Balanced Specification, Cross Validation, and Optimal Lambda/Deviance Extraction

```{r}
library(glmnet)

# Create predictor matrix (x2) and outcome vector (y2) from balanced dataset
x2 <- model.matrix(Revenue ~ ProductRelated_Duration + BounceRates + ExitRates +
                   VisitorType + Weekend + Month, data = Shopping.bal)[, -1]
y2 <- Shopping.bal$Revenue

# Fit LASSO model
lasso_model2 <- glmnet(x2, y2, alpha = 1, family = "binomial")
plot(lasso_model2, main = "LASSO Coefficient Path - Spec 2 (Balanced)")

# Cross-validation
set.seed(1)
lasso_cv2 <- cv.glmnet(x2, y2, alpha = 1, family = "binomial")
plot(lasso_cv2, main = "LASSO CV Error Plot - Spec 2 (Balanced)")

# Extract optimal lambda and deviance
lasso_lambda2 <- lasso_cv2$lambda.min
lasso_coef2 <- coef(lasso_cv2, s = lasso_lambda2)

round(cbind("Best Lambda" = lasso_lambda2,
            "Best Log Lambda" = log(lasso_lambda2),
            "Best 10FCV" = min(lasso_cv2$cvm)), 5)

```

List Results

```{r}
# For Spec 1 (Unbalanced)
lasso_lambda1 <- lasso_cv1$lambda.min
min_cv1 <- min(lasso_cv1$cvm)

# For Spec 2 (Balanced)
lasso_lambda2 <- lasso_cv2$lambda.min
min_cv2 <- min(lasso_cv2$cvm)

lasso.results <- round(
  rbind(
    "Spec 1: Unbalanced" = c("Best Lambda" = lasso_lambda1,
                             "Best Log Lambda" = log(lasso_lambda1),
                             "Best 10FCV" = min_cv1),
    "Spec 2: Balanced"   = c("Best Lambda" = lasso_lambda2,
                             "Best Log Lambda" = log(lasso_lambda2),
                             "Best 10FCV" = min_cv2)
  ), 5
)

lasso.results


```




Bagging and Random Forest
```{r}
# Set seed and RNG setting
RNGkind(sample.kind = "default")
set.seed(1)

# Load library
library(randomForest)

# Ensure binary outcome is a factor with labels
Shopping$Revenue <- factor(Shopping$Revenue, levels = c(FALSE, TRUE), labels = c("No", "Yes"))
Shopping.bal$Revenue <- factor(Shopping.bal$Revenue, levels = c(FALSE, TRUE), labels = c("No", "Yes"))

# Define modeling formula
model_formula <- Revenue ~ ProductRelated_Duration + BounceRates + ExitRates + 
                              VisitorType + Weekend + Month

# Create NA-safe subsets for modeling
Shopping.clean <- na.omit(Shopping[, all.vars(model_formula)])
Shopping.bal.clean <- na.omit(Shopping.bal[, all.vars(model_formula)])

# -------------------------------
# 1. Bagging (Unbalanced, mtry = 6)
# -------------------------------
bag.unbal <- randomForest(model_formula,
                          data = Shopping.clean,  
                          mtry = 6,  
                          importance = TRUE)

print(bag.unbal)
cat("\nClassification Error Rate (Bagging - Unbalanced) =",
    bag.unbal$err.rate[nrow(bag.unbal$err.rate), "OOB"])

varImpPlot(bag.unbal, type = 2)
importance(bag.unbal, type = 2)

# -------------------------------
# 2. Bagging (Balanced, mtry = 6)
# -------------------------------
bag.bal <- randomForest(model_formula,
                        data = Shopping.bal.clean,  
                        mtry = 6,  
                        importance = TRUE)

print(bag.bal)
cat("\nClassification Error Rate (Bagging - Balanced) =",
    bag.bal$err.rate[nrow(bag.bal$err.rate), "OOB"])

varImpPlot(bag.bal, type = 2)
importance(bag.bal, type = 2)

# -------------------------------
# 3. Random Forest (Unbalanced, mtry = 3)
# -------------------------------
rf.unbal <- randomForest(model_formula,
                         data = Shopping.clean,  
                         mtry = 3,  
                         importance = TRUE)

print(rf.unbal)
cat("\nClassification Error Rate (Random Forest - Unbalanced) =",
    rf.unbal$err.rate[nrow(rf.unbal$err.rate), "OOB"])

varImpPlot(rf.unbal, type = 2)
importance(rf.unbal, type = 2)

# -------------------------------
# 4. Random Forest (Balanced, mtry = 3)
# -------------------------------
rf.bal <- randomForest(model_formula,
                       data = Shopping.bal.clean,  
                       mtry = 3,  
                       importance = TRUE)

print(rf.bal)
cat("\nClassification Error Rate (Random Forest - Balanced) =",
    rf.bal$err.rate[nrow(rf.bal$err.rate), "OOB"])

varImpPlot(rf.bal, type = 2)
importance(rf.bal, type = 2)

```


#10FCV For Logit, LASSO ,Random Forest, and Bagging models
```
# Load caret and randomForest
library(caret)
library(randomForest)

# Define formula (already established)
model_formula <- Revenue ~ ProductRelated_Duration + BounceRates + ExitRates + VisitorType + Weekend + Month

# Set control for 10-fold cross-validation
ctrl <- trainControl(method = "cv", number = 10)

# 1. Bagging - Unbalanced
set.seed(1)
bag_cv_unbal <- train(model_formula, data = Shopping.clean,
                      method = "treebag", trControl = ctrl)

# 2. Bagging - Balanced
set.seed(1)
bag_cv_bal <- train(model_formula, data = Shopping.bal.clean,
                    method = "treebag", trControl = ctrl)

# 3. Random Forest - Unbalanced
set.seed(1)
rf_cv_unbal <- train(model_formula, data = Shopping.clean,
                     method = "rf", trControl = ctrl, tuneGrid = data.frame(mtry = 3))

# 4. Random Forest - Balanced
set.seed(1)
rf_cv_bal <- train(model_formula, data = Shopping.bal.clean,
                   method = "rf", trControl = ctrl, tuneGrid = data.frame(mtry = 3))

```
{r}
# Extract and convert accuracy to error rate
bag_cv_unbal_error <- 1 - max(bag_cv_unbal$results$Accuracy)
bag_cv_bal_error   <- 1 - max(bag_cv_bal$results$Accuracy)
rf_cv_unbal_error  <- 1 - max(rf_cv_unbal$results$Accuracy)
rf_cv_bal_error    <- 1 - max(rf_cv_bal$results$Accuracy)

```

{r}
library(boot)

# Logistic Model – Spec 1: Unbalanced
set.seed(1)
cv_logit1 <- cv.glm(data = Shopping, glmfit = logit_model, K = 10)

# Logistic Model – Spec 2: Balanced
set.seed(1)
cv_logit2 <- cv.glm(data = Shopping.bal, glmfit = logit_model_bal, K = 10)

```

{r}
model_comparison <- data.frame(
  Model = c("Logit Spec 1 (Unbalanced)", "Logit Spec 2 (Balanced)",
            "LASSO Spec 1 (Unbalanced)", "LASSO Spec 2 (Balanced)",
            "Bagging - Unbalanced", "Bagging - Balanced",
            "Random Forest - Unbalanced", "Random Forest - Balanced"),
  `10FCV Error` = round(c(logit_cv_error1, logit_cv_error2,
                          lasso_cv_error1, lasso_cv_error2,
                          bag_cv_unbal_error, bag_cv_bal_error,
                          rf_cv_unbal_error, rf_cv_bal_error), 5)
)

# View
model_comparison

```
